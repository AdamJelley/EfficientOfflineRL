{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_actions = np.array([[0,1,0],[1,0,1]])\n",
    "\n",
    "policy = np.array([[0,1,0],[0.5,0,0.5]])\n",
    "\n",
    "rewards = np.array([[0,0,0],[0,0,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_state_values(policy, rewards, gamma=0.9, theta=0.01):\n",
    "  \"\"\"Evaluate values of states for given policy and reward mappings.\n",
    "\n",
    "  Args:\n",
    "      policy (np.array): Policy giving the probability of taking each action from each state.\n",
    "      rewards (np.array): Rewards corresponding to reaching the next state from each state (so r(s,s') here rather than the usual r(s,a)).\n",
    "      gamma (float, optional): Discount factor. Defaults to 0.9.\n",
    "      theta (float, optional): Minimum value change threshold to terminate iteration. Defaults to 0.01.\n",
    "\n",
    "  Returns:\n",
    "      values (np.array): State values for given policy and reward mappings.\n",
    "  \"\"\"    \n",
    "\n",
    "  num_states, num_actions = policy.shape # 2, 3\n",
    "  #values = np.zeros((num_states+1)) # Include terminal state\n",
    "  values = np.array([10*gamma**3, 0.5*(10*gamma**2+10), 0])\n",
    "\n",
    "  print(f'Beginning policy evaluation for given policy and MDP...\\n')\n",
    "  iteration = 1\n",
    "\n",
    "  while True:\n",
    "    print(f'Iteration: {iteration} \\t Current Values: {values}')\n",
    "    delta=0\n",
    "    initial_values = values\n",
    "    values = np.zeros_like(values)\n",
    "    for state in range(num_states):\n",
    "      for action in range(num_actions):\n",
    "        # With probability 0.9, next state = action with corresponding reward. With probability 0.1, next state = state with no reward.\n",
    "        next_state_probabilities = {action:1.0, state:0.0}\n",
    "        # Often we leave this next state computation (transition dynamics) to the environment. But to use complete dynamic programming we must know \n",
    "        # the transition probablities.\n",
    "        for next_state in next_state_probabilities.keys():\n",
    "          # Note the expectation here is over both the policy and next state probabilities (environment dynamics)\n",
    "          values[state] += policy[state][action]*next_state_probabilities[next_state]*(rewards[state][next_state]+gamma*initial_values[next_state])\n",
    "\n",
    "      delta = max(delta, abs(initial_values[state]-values[state]))\n",
    "\n",
    "    if delta < theta:\n",
    "      print(f'\\nMax difference in state value from previous iteration = {delta} which is less than threshold {theta}. Policy Evaluation terminating...\\n')\n",
    "      break\n",
    "\n",
    "    iteration+=1\n",
    "\n",
    "  print(f'Final policy state values: {values}')\n",
    "  return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning policy evaluation for given policy and MDP...\n",
      "\n",
      "Iteration: 1 \t Current Values: [0. 0. 0.]\n",
      "Iteration: 2 \t Current Values: [0. 5. 0.]\n",
      "Iteration: 3 \t Current Values: [4.5 5.  0. ]\n",
      "Iteration: 4 \t Current Values: [4.5   7.025 0.   ]\n",
      "Iteration: 5 \t Current Values: [6.3225 7.025  0.    ]\n",
      "Iteration: 6 \t Current Values: [6.3225   7.845125 0.      ]\n",
      "Iteration: 7 \t Current Values: [7.0606125 7.845125  0.       ]\n",
      "Iteration: 8 \t Current Values: [7.0606125  8.17727563 0.        ]\n",
      "Iteration: 9 \t Current Values: [7.35954806 8.17727563 0.        ]\n",
      "Iteration: 10 \t Current Values: [7.35954806 8.31179663 0.        ]\n",
      "Iteration: 11 \t Current Values: [7.48061697 8.31179663 0.        ]\n",
      "Iteration: 12 \t Current Values: [7.48061697 8.36627763 0.        ]\n",
      "Iteration: 13 \t Current Values: [7.52964987 8.36627763 0.        ]\n",
      "Iteration: 14 \t Current Values: [7.52964987 8.38834244 0.        ]\n",
      "Iteration: 15 \t Current Values: [7.5495082  8.38834244 0.        ]\n",
      "\n",
      "Max difference in state value from previous iteration = 0.008936247052718116 which is less than threshold 0.01. Policy Evaluation terminating...\n",
      "\n",
      "Final policy state values: [7.5495082  8.39727869 0.        ]\n"
     ]
    }
   ],
   "source": [
    "values = calculate_state_values(policy, rewards, gamma=0.9, theta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning policy evaluation for given policy and MDP...\n",
      "\n",
      "Iteration: 1 \t Current Values: [7.29 9.05 0.  ]\n",
      "Iteration: 2 \t Current Values: [8.145  8.2805 0.    ]\n",
      "Iteration: 3 \t Current Values: [7.45245 8.66525 0.     ]\n",
      "Iteration: 4 \t Current Values: [7.798725  8.3536025 0.       ]\n",
      "Iteration: 5 \t Current Values: [7.51824225 8.50942625 0.        ]\n",
      "Iteration: 6 \t Current Values: [7.65848363 8.38320901 0.        ]\n",
      "Iteration: 7 \t Current Values: [7.54488811 8.44631763 0.        ]\n",
      "Iteration: 8 \t Current Values: [7.60168587 8.39519965 0.        ]\n",
      "Iteration: 9 \t Current Values: [7.55567969 8.42075864 0.        ]\n",
      "Iteration: 10 \t Current Values: [7.57868278 8.40005586 0.        ]\n",
      "Iteration: 11 \t Current Values: [7.56005027 8.41040725 0.        ]\n",
      "\n",
      "Max difference in state value from previous iteration = 0.009316252071421616 which is less than threshold 0.01. Policy Evaluation terminating...\n",
      "\n",
      "Final policy state values: [7.56936652 8.40202262 0.        ]\n"
     ]
    }
   ],
   "source": [
    "values = calculate_state_values(policy, rewards, gamma=0.9, theta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-04-11-10:54:41'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
